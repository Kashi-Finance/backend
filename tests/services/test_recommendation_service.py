"""
Comprehensive tests for the Recommendation Service.

These tests verify the recommendation service behavior including:
- Basic product queries with various budgets
- User notes and preferences
- Contradictory fields (query vs user_note)
- Edge cases and boundary conditions
- Language extraction from locale
- Prohibited content handling
- Out-of-scope queries
- Store preferences

Note: These tests use mocked Gemini responses to avoid actual API calls
and ensure deterministic test behavior.
"""

import pytest
from decimal import Decimal
from unittest.mock import MagicMock, patch, AsyncMock
from typing import Dict, Any

from backend.services.recommendation_service import (
    query_recommendations,
    _extract_language_from_locale,
    _validate_llm_response,
)
from backend.schemas.recommendations import (
    RecommendationQueryResponseOK,
    RecommendationQueryResponseNoValidOption,
    ProductRecommendation,
)
from backend.agents.recommendation.prompts import build_recommendation_user_prompt


# =============================================================================
# FIXTURES
# =============================================================================

@pytest.fixture
def mock_supabase_client_gt():
    """Mock Supabase client for Guatemala user."""
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_response.data = [{
        "country": "GT",
        "currency_preference": "GTQ",
        "locale": "es-GT"
    }]
    mock_client.table.return_value.select.return_value.eq.return_value.execute.return_value = mock_response
    return mock_client


@pytest.fixture
def mock_supabase_client_us():
    """Mock Supabase client for US user."""
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_response.data = [{
        "country": "US",
        "currency_preference": "USD",
        "locale": "en-US"
    }]
    mock_client.table.return_value.select.return_value.eq.return_value.execute.return_value = mock_response
    return mock_client


@pytest.fixture
def mock_supabase_client_system_locale():
    """Mock Supabase client with 'system' locale (should infer from country)."""
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_response.data = [{
        "country": "MX",
        "currency_preference": "MXN",
        "locale": "system"
    }]
    mock_client.table.return_value.select.return_value.eq.return_value.execute.return_value = mock_response
    return mock_client


@pytest.fixture
def mock_gemini_ok_response():
    """Mock Gemini response with valid products."""
    return {
        "status": "OK",
        "products": [
            {
                "product_title": "Test Laptop 15 inch",
                "price_total": 5999.00,
                "seller_name": "Test Store",
                "url": "https://example.com/laptop",
                "pickup_available": True,
                "warranty_info": "1 year warranty",
                "copy_for_user": "Great laptop for everyday use.",
                "badges": ["Fast", "Reliable", "Affordable"]
            },
            {
                "product_title": "Test Laptop Pro",
                "price_total": 7500.00,
                "seller_name": "Another Store",
                "url": "https://example.com/laptop-pro",
                "pickup_available": False,
                "warranty_info": "2 year warranty",
                "copy_for_user": "Professional grade laptop.",
                "badges": ["Pro", "High Performance", "Premium"]
            }
        ],
        "metadata": {
            "total_results": 2,
            "query_understood": True,
            "search_successful": True
        }
    }


@pytest.fixture
def mock_gemini_no_valid_option_response():
    """Mock Gemini response for NO_VALID_OPTION."""
    return {
        "status": "NO_VALID_OPTION",
        "products": [],
        "metadata": {
            "total_results": 0,
            "query_understood": True,
            "search_successful": True,
            "reason": "No products found within budget."
        }
    }


@pytest.fixture
def mock_gemini_prohibited_response():
    """Mock Gemini response for prohibited content."""
    return {
        "status": "NO_VALID_OPTION",
        "products": [],
        "metadata": {
            "total_results": 0,
            "query_understood": False,
            "search_successful": False,
            "reason": "This request involves prohibited content."
        }
    }


# =============================================================================
# UNIT TESTS: Language Extraction
# =============================================================================

class TestLanguageExtraction:
    """Tests for _extract_language_from_locale function."""
    
    def test_spanish_guatemala(self):
        """es-GT should return Spanish."""
        result = _extract_language_from_locale("es-GT", "GT")
        assert result == "Spanish"
    
    def test_spanish_mexico(self):
        """es-MX should return Spanish."""
        result = _extract_language_from_locale("es-MX", "MX")
        assert result == "Spanish"
    
    def test_english_us(self):
        """en-US should return English."""
        result = _extract_language_from_locale("en-US", "US")
        assert result == "English"
    
    def test_english_uk(self):
        """en-GB should return English."""
        result = _extract_language_from_locale("en-GB", "GB")
        assert result == "English"
    
    def test_portuguese_brazil(self):
        """pt-BR should return Portuguese."""
        result = _extract_language_from_locale("pt-BR", "BR")
        assert result == "Portuguese"
    
    def test_system_locale_guatemala(self):
        """'system' locale with GT country should return Spanish."""
        result = _extract_language_from_locale("system", "GT")
        assert result == "Spanish"
    
    def test_system_locale_us(self):
        """'system' locale with US country should return English."""
        result = _extract_language_from_locale("system", "US")
        assert result == "English"
    
    def test_system_locale_brazil(self):
        """'system' locale with BR country should return Portuguese."""
        result = _extract_language_from_locale("system", "BR")
        assert result == "Portuguese"
    
    def test_unknown_locale_defaults_to_spanish(self):
        """Unknown locale should default to Spanish (Latin America focus)."""
        result = _extract_language_from_locale("xx-YY", "YY")
        assert result == "Spanish"
    
    def test_case_insensitive_locale(self):
        """Locale extraction should be case-insensitive."""
        result = _extract_language_from_locale("EN-US", "US")
        assert result == "English"
    
    def test_system_case_insensitive(self):
        """'system' check should be case-insensitive."""
        result = _extract_language_from_locale("SYSTEM", "US")
        assert result == "English"


# =============================================================================
# UNIT TESTS: Response Validation
# =============================================================================

class TestResponseValidation:
    """Tests for _validate_llm_response function."""
    
    def test_valid_ok_response(self, mock_gemini_ok_response):
        """Valid OK response should pass validation."""
        assert _validate_llm_response(mock_gemini_ok_response) is True
    
    def test_valid_no_valid_option_response(self, mock_gemini_no_valid_option_response):
        """Valid NO_VALID_OPTION response should pass validation."""
        assert _validate_llm_response(mock_gemini_no_valid_option_response) is True
    
    def test_missing_status(self):
        """Response without status should fail."""
        response = {"products": [], "metadata": {}}
        assert _validate_llm_response(response) is False
    
    def test_missing_products(self):
        """Response without products should fail."""
        response = {"status": "OK", "metadata": {}}
        assert _validate_llm_response(response) is False
    
    def test_missing_metadata(self):
        """Response without metadata should fail."""
        response = {"status": "OK", "products": []}
        assert _validate_llm_response(response) is False
    
    def test_invalid_status(self):
        """Response with invalid status should fail."""
        response = {"status": "INVALID", "products": [], "metadata": {}}
        assert _validate_llm_response(response) is False
    
    def test_ok_with_empty_products(self):
        """OK status with empty products should fail."""
        response = {"status": "OK", "products": [], "metadata": {}}
        assert _validate_llm_response(response) is False
    
    def test_product_missing_required_field(self):
        """Product missing required field should fail."""
        response = {
            "status": "OK",
            "products": [{"product_title": "Test"}],  # Missing other fields
            "metadata": {"total_results": 1}
        }
        assert _validate_llm_response(response) is False


# =============================================================================
# UNIT TESTS: Prompt Building
# =============================================================================

class TestPromptBuilding:
    """Tests for build_recommendation_user_prompt function."""
    
    def test_basic_prompt_includes_query(self):
        """Prompt should include the user's query."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop para dise침o",
            country="GT",
            currency="GTQ",
        )
        assert "laptop para dise침o" in prompt
    
    def test_prompt_includes_country(self):
        """Prompt should include country."""
        prompt = build_recommendation_user_prompt(
            query_raw="test",
            country="MX",
            currency="MXN",
        )
        assert "MX" in prompt
    
    def test_prompt_includes_currency(self):
        """Prompt should include currency."""
        prompt = build_recommendation_user_prompt(
            query_raw="test",
            country="GT",
            currency="GTQ",
        )
        assert "GTQ" in prompt
    
    def test_prompt_includes_budget(self):
        """Prompt should include budget when provided."""
        prompt = build_recommendation_user_prompt(
            query_raw="test",
            country="GT",
            currency="GTQ",
            budget_hint=5000.0,
        )
        assert "5000.00 GTQ" in prompt
    
    def test_prompt_includes_language(self):
        """Prompt should include language instruction."""
        prompt = build_recommendation_user_prompt(
            query_raw="test",
            country="GT",
            currency="GTQ",
            language="Spanish",
        )
        assert "Respond in Spanish" in prompt
    
    def test_prompt_includes_english_for_us(self):
        """Prompt should include English for US users."""
        prompt = build_recommendation_user_prompt(
            query_raw="test",
            country="US",
            currency="USD",
            language="English",
        )
        assert "Respond in English" in prompt
    
    def test_prompt_includes_user_note(self):
        """Prompt should include user_note when provided."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop",
            country="GT",
            currency="GTQ",
            user_note="No quiero RGB ni dise침o gamer",
        )
        assert "No quiero RGB ni dise침o gamer" in prompt
        assert "<user_preferences>" in prompt
    
    def test_prompt_includes_preferred_store(self):
        """Prompt should include preferred_store when provided."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop",
            country="GT",
            currency="GTQ",
            preferred_store="Tiendas MAX",
        )
        assert "Tiendas MAX" in prompt
    
    def test_prompt_no_budget_shows_not_specified(self):
        """Prompt should show 'Not specified' when no budget."""
        prompt = build_recommendation_user_prompt(
            query_raw="test",
            country="GT",
            currency="GTQ",
        )
        assert "Not specified" in prompt
    
    def test_prompt_includes_extra_details(self):
        """Prompt should include extra_details when provided."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop",
            country="GT",
            currency="GTQ",
            extra_details={
                "screen_size": "15 pulgadas",
                "ram": "16GB m칤nimo"
            },
        )
        assert "screen_size: 15 pulgadas" in prompt
        assert "ram: 16GB m칤nimo" in prompt


# =============================================================================
# UNIT TESTS: Complex Query Scenarios
# =============================================================================

class TestComplexQueryScenarios:
    """Tests for complex real-world query scenarios with user_note."""
    
    def test_prompt_with_conflicting_query_and_note(self):
        """
        Test when query_raw and user_note have conflicting information.
        Example: Query asks for gaming laptop, note says no RGB/gamer design.
        """
        prompt = build_recommendation_user_prompt(
            query_raw="laptop gaming potente con buena tarjeta gr치fica",
            country="GT",
            currency="GTQ",
            budget_hint=10000.0,
            user_note="No quiero dise침o gamer con luces RGB, prefiero algo sobrio para oficina",
        )
        # Both should be present - LLM should reconcile
        assert "laptop gaming potente" in prompt
        assert "No quiero dise침o gamer" in prompt
    
    def test_prompt_with_specific_brand_preference(self):
        """Test with specific brand preference in user_note."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop para programaci칩n",
            country="GT",
            currency="GTQ",
            budget_hint=8000.0,
            user_note="Prefiero marcas como Lenovo ThinkPad o Dell XPS, no HP ni Acer",
        )
        assert "Lenovo ThinkPad" in prompt
        assert "Dell XPS" in prompt
    
    def test_prompt_with_specific_feature_requirements(self):
        """Test with specific feature requirements in user_note."""
        prompt = build_recommendation_user_prompt(
            query_raw="refrigeradora",
            country="GT",
            currency="GTQ",
            budget_hint=6000.0,
            user_note="Necesito que tenga dispensador de agua y hielo, m칤nimo 18 pies c칰bicos, preferiblemente color acero inoxidable",
        )
        assert "dispensador de agua" in prompt
        assert "18 pies c칰bicos" in prompt
    
    def test_prompt_with_negative_constraints(self):
        """Test with negative constraints (things to avoid)."""
        prompt = build_recommendation_user_prompt(
            query_raw="aud칤fonos inal치mbricos",
            country="GT",
            currency="GTQ",
            budget_hint=800.0,
            user_note="NO quiero earbuds, solo over-ear. No marcas chinas desconocidas. Que tenga cancelaci칩n de ruido activa.",
        )
        assert "NO quiero earbuds" in prompt
        assert "over-ear" in prompt
        assert "cancelaci칩n de ruido" in prompt
    
    def test_prompt_with_use_case_context(self):
        """Test with detailed use case in user_note."""
        prompt = build_recommendation_user_prompt(
            query_raw="monitor para computadora",
            country="GT",
            currency="GTQ",
            budget_hint=3500.0,
            user_note="Es para trabajo de dise침o gr치fico y edici칩n de fotos, necesito buena reproducci칩n de colores, m칤nimo 100% sRGB, no necesita ser gamer",
        )
        assert "dise침o gr치fico" in prompt
        assert "100% sRGB" in prompt
    
    def test_prompt_with_urgency_and_availability(self):
        """Test with urgency and availability constraints."""
        prompt = build_recommendation_user_prompt(
            query_raw="impresora l치ser",
            country="GT",
            currency="GTQ",
            budget_hint=2000.0,
            user_note="La necesito urgente, tiene que estar disponible para recoger hoy o ma침ana en zona 10 o zona 15 de Guatemala",
            preferred_store="Intelaf",
        )
        assert "urgente" in prompt
        assert "zona 10" in prompt
        assert "Intelaf" in prompt
    
    def test_prompt_with_gift_context(self):
        """Test with gift purchase context."""
        prompt = build_recommendation_user_prompt(
            query_raw="smartwatch",
            country="GT",
            currency="GTQ",
            budget_hint=2500.0,
            user_note="Es para regalo de una mujer de 30 a침os, debe verse elegante, no muy deportivo. Compatibilidad con iPhone es importante.",
        )
        assert "regalo" in prompt
        assert "elegante" in prompt
        assert "iPhone" in prompt
    
    def test_prompt_with_contradictory_budget_expectations(self):
        """Test when query expects high-end but budget is limited."""
        prompt = build_recommendation_user_prompt(
            query_raw="iPhone 15 Pro Max nuevo",
            country="GT",
            currency="GTQ",
            budget_hint=500.0,  # Way too low for an iPhone Pro Max
            user_note="Tiene que ser nuevo, sellado, con garant칤a Apple oficial",
        )
        assert "iPhone 15 Pro Max" in prompt
        assert "500.00 GTQ" in prompt
        assert "nuevo, sellado" in prompt


# =============================================================================
# UNIT TESTS: Edge Cases
# =============================================================================

class TestEdgeCases:
    """Tests for edge cases and boundary conditions."""
    
    def test_empty_query(self):
        """Test with empty query string."""
        prompt = build_recommendation_user_prompt(
            query_raw="",
            country="GT",
            currency="GTQ",
        )
        assert "<query>" in prompt
        assert "</query>" in prompt
    
    def test_very_long_query(self):
        """Test with very long query string."""
        long_query = "laptop " * 100
        prompt = build_recommendation_user_prompt(
            query_raw=long_query,
            country="GT",
            currency="GTQ",
        )
        assert long_query.strip() in prompt
    
    def test_query_with_special_characters(self):
        """Test with special characters in query."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop < 7000 & >= 5000",
            country="GT",
            currency="GTQ",
        )
        # Should not break the prompt
        assert "laptop < 7000 & >= 5000" in prompt
    
    def test_query_with_emojis(self):
        """Test with emojis in query."""
        prompt = build_recommendation_user_prompt(
            query_raw="游꿡 laptop gaming 游댠",
            country="GT",
            currency="GTQ",
        )
        assert "游꿡" in prompt
        assert "游댠" in prompt
    
    def test_zero_budget(self):
        """Test with zero budget - should show 'Not specified' since 0.0 is falsy."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop",
            country="GT",
            currency="GTQ",
            budget_hint=0.0,
        )
        # 0.0 is falsy in Python, so budget shows as "Not specified"
        assert "Not specified" in prompt
    
    def test_very_large_budget(self):
        """Test with very large budget."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop",
            country="GT",
            currency="GTQ",
            budget_hint=999999999.99,
        )
        assert "999999999.99 GTQ" in prompt
    
    def test_user_note_with_html_like_content(self):
        """Test user_note that looks like HTML (shouldn't break XML structure)."""
        prompt = build_recommendation_user_prompt(
            query_raw="laptop",
            country="GT",
            currency="GTQ",
            user_note="<script>alert('xss')</script>",
        )
        # Should be included as-is (model handles it)
        assert "<script>" in prompt


# =============================================================================
# INTEGRATION TESTS: Mocked Gemini API
# =============================================================================

class TestRecommendationServiceIntegration:
    """Integration tests with mocked Gemini API."""
    
    @pytest.mark.asyncio
    async def test_successful_recommendation_query(
        self, mock_supabase_client_gt, mock_gemini_ok_response
    ):
        """Test successful recommendation query returns products."""
        with patch('backend.services.recommendation_service._get_gemini_client') as mock_client:
            # Setup mock Gemini response
            mock_response = MagicMock()
            mock_response.text = str(mock_gemini_ok_response).replace("'", '"').replace("True", "true").replace("False", "false")
            mock_response.candidates = [MagicMock()]
            mock_response.candidates[0].content = MagicMock()
            mock_response.candidates[0].grounding_metadata = None
            
            mock_gemini = MagicMock()
            mock_gemini.models.generate_content.return_value = mock_response
            mock_client.return_value = mock_gemini
            
            import json
            mock_response.text = json.dumps(mock_gemini_ok_response)
            
            result = await query_recommendations(
                supabase_client=mock_supabase_client_gt,
                user_id="test-user-123",
                query_raw="laptop para dise침o",
                budget_hint=Decimal("7000"),
            )
            
            assert result.status == "OK"
            assert isinstance(result, RecommendationQueryResponseOK)
            assert len(result.results_for_user) == 2
    
    @pytest.mark.asyncio
    async def test_no_valid_option_response(
        self, mock_supabase_client_gt, mock_gemini_no_valid_option_response
    ):
        """Test NO_VALID_OPTION response handling."""
        with patch('backend.services.recommendation_service._get_gemini_client') as mock_client:
            import json
            mock_response = MagicMock()
            mock_response.text = json.dumps(mock_gemini_no_valid_option_response)
            mock_response.candidates = [MagicMock()]
            mock_response.candidates[0].content = MagicMock()
            mock_response.candidates[0].grounding_metadata = None
            
            mock_gemini = MagicMock()
            mock_gemini.models.generate_content.return_value = mock_response
            mock_client.return_value = mock_gemini
            
            result = await query_recommendations(
                supabase_client=mock_supabase_client_gt,
                user_id="test-user-123",
                query_raw="laptop gaming profesional",
                budget_hint=Decimal("100"),  # Too low
            )
            
            assert result.status == "NO_VALID_OPTION"
            assert isinstance(result, RecommendationQueryResponseNoValidOption)
            assert result.reason is not None
    
    @pytest.mark.asyncio
    async def test_gemini_client_not_configured(self, mock_supabase_client_gt):
        """Test handling when Gemini client is not configured."""
        with patch('backend.services.recommendation_service._get_gemini_client') as mock_client:
            mock_client.return_value = None  # Client not configured
            
            result = await query_recommendations(
                supabase_client=mock_supabase_client_gt,
                user_id="test-user-123",
                query_raw="laptop",
            )
            
            assert result.status == "NO_VALID_OPTION"
            assert result.reason is not None
            assert "not configured" in result.reason.lower()
    
    @pytest.mark.asyncio
    async def test_profile_not_found_uses_defaults(self):
        """Test that missing profile uses default values."""
        mock_client = MagicMock()
        mock_response = MagicMock()
        mock_response.data = []  # No profile found
        mock_client.table.return_value.select.return_value.eq.return_value.execute.return_value = mock_response
        
        with patch('backend.services.recommendation_service._get_gemini_client') as mock_gemini_client:
            mock_gemini_client.return_value = None
            
            result = await query_recommendations(
                supabase_client=mock_client,
                user_id="test-user-123",
                query_raw="laptop",
            )
            
            # Should still work with defaults (GT, GTQ, es-GT)
            assert result.status == "NO_VALID_OPTION"  # Because client is None


# =============================================================================
# REAL WORLD QUERY TESTS (PARAMETERIZED)
# =============================================================================

class TestRealWorldQueries:
    """
    Parameterized tests for real-world query scenarios.
    These test the prompt building for various common user queries.
    """
    
    @pytest.mark.parametrize("query,budget,user_note,expected_in_prompt", [
        # Electronics
        (
            "laptop para estudiante universitario",
            5000.0,
            "Que sea liviana para llevar a clases, buena bater칤a",
            ["estudiante", "liviana", "bater칤a"]
        ),
        (
            "tablet para ni침o de 8 a침os",
            1500.0,
            "Que sea resistente a golpes, control parental, no muy cara porque los ni침os las rompen",
            ["ni침o", "resistente", "control parental"]
        ),
        (
            "c치mara para fotograf칤a profesional",
            15000.0,
            "Full frame, buena para retratos y paisajes, compatible con lentes Canon EF",
            ["Full frame", "Canon EF", "retratos"]
        ),
        # Home appliances
        (
            "lavadora de ropa",
            4000.0,
            "Carga frontal, capacidad para familia de 5 personas, eficiente en agua",
            ["Carga frontal", "familia de 5", "eficiente"]
        ),
        (
            "aire acondicionado para cuarto de 4x4 metros",
            3500.0,
            "Inverter, que sea silencioso, marca reconocida",
            ["4x4 metros", "Inverter", "silencioso"]
        ),
        # Contradictory scenarios
        (
            "auto deportivo de lujo",
            50.0,
            "Que sea nuevo y con garant칤a de agencia",
            ["auto deportivo", "50.00", "nuevo"]  # Obviously impossible budget
        ),
        (
            "laptop gaming barata",
            8000.0,
            "NO quiero que sea gamer, prefiero algo elegante para oficina",
            ["laptop gaming", "NO quiero que sea gamer", "elegante"]  # Contradiction
        ),
    ])
    def test_real_world_prompt_content(self, query, budget, user_note, expected_in_prompt):
        """Test that prompts contain expected content for real-world queries."""
        prompt = build_recommendation_user_prompt(
            query_raw=query,
            country="GT",
            currency="GTQ",
            budget_hint=budget,
            user_note=user_note,
        )
        
        for expected in expected_in_prompt:
            assert expected in prompt, f"Expected '{expected}' in prompt"
